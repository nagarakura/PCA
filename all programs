1.mean-medimm
--------------

data = [115.3, 195.5, 120.5, 110.2, 90.4, 105.6, 110.9, 116.3, 122.3, 125.4]

mean = sum(data) / len(data)

print(mean)

sorted_data = sorted(data)
n = len(sorted_data)
if n % 2 == 0:
    median = (sorted_data[n // 2 - 1] + sorted_data[n // 2]) / 2
else:
    median = sorted_data[n // 2]


print(median)

from collections import Counter
counter = Counter(data)
mode = [k for k, v in counter.items() if v == max(counter.values())]


print(mode)

mean_diff_squared = [(x - mean) ** 2 for x in data]
variance = sum(mean_diff_squared) / len(data)
std_deviation = variance ** 0.5

print(std_deviation)

print(variance)

min_value = min(data)
max_value = max(data)
normalized_data = [(x - min_value) / (max_value - min_value) for x in data]
print(normalized_data)

mean_normalized = [(x - mean) for x in data]
std_deviation_normalized = [(x - mean) / std_deviation for x in data]

print(std_deviation_normalized)


------------
2.naiveB
--------------

import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report,roc_curve
from sklearn.model_selection import train_test_split

data = pd.read_csv(r"C:\Users\amana\Downloads\covid.csv")

le = preprocessing.LabelEncoder()
pc = le.fit_transform(data['pc'].values)
wbc = le.fit_transform(data['wbc'].values)
mc = le.fit_transform(data['mc'].values)
ast = le.fit_transform(data['ast'].values)
bc = le.fit_transform(data['bc'].values)
ldh = le.fit_transform(data['ldh'].values)
y = le.fit_transform(data['diagnosis'].values)
X = np.array(list(zip(pc, wbc, mc, ast, bc, ldh)))

xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25)

naivee = MultinomialNB()
naivee.fit(xtrain, ytrain)
ypred = naivee.predict(xtest)
print("Accuracy: ", accuracy_score(ytest, ypred))
print("Classification Report: \n", classification_report(ytest, ypred))

lr_probs = naivee.predict_proba(xtest)[:,1]
lr_fpr, lr_tpr, _=roc_curve(ytest, lr_probs)
from matplotlib import pyplot
pyplot.plot(lr_fpr, lr_tpr,label='Naive Bayes Classifier')

pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
pyplot.legend()
pyplot.show()


-----------
3.svm
----------
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import statistics as st

df=pd.read_csv("Food-Truck.csv",names=['a','b'])
x=df['a']
y=df['b']
x_mean=st.mean(x);
y_mean=st.mean(y)
plt.scatter(x,y)


n=0
d=0
for i in range(len(x)):
 n+=(x[i]-x_mean)*(y[i]-y_mean)
 d+=(x[i]-x_mean)**2
m=n/d
c=y_mean-m*x_mean
line=[]
for i in x:
 line.append(m*i+c)
plt.scatter(x,y,label='points',color='r')
plt.plot(x,line,label='line')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()


nm=st.mean(line)
sse=0
ssr=0
sst=0
for i in range(len(y)):
 sse+=(y[i]-line[i])**2
for i in range(len(line)):
 ssr+=(line[i]-nm)**2
for i in range(len(y)):
 sst+=(y[i]-y_mean)**2
r_sq=1-(sse/sst)
print(sse,ssr,sst,r_sq)

--------------
4.DT
_______________
from sklearn.metrics import classification_report, accuracy_score
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier,plot_tree

df=pd.read_csv(r"C:\Users\amana\Downloads\zoo_data(For Decision Tree Program).csv")
df

 X=df.drop('1.7',axis=1)
 Y=df["1.7"]
 x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.3,random_state=100)

clf_entropy=DecisionTreeClassifier(criterion='entropy',random_state=100,max_depth=3,min_samples_leaf=5)
clf_entropy.fit(x_train,y_train)
y_pred=clf_entropy.predict(x_test)

print('confusion matrix: ',confusion_matrix(y_test,y_pred))
print('classification report: ',classification_report(y_test,y_pred))
print('accuracy: ',accuracy_score(y_test,y_pred)*100)

plt.figure(figsize=(20, 15))
plot_tree( clf_entropy, filled=True)
plt.show()

------------------
5.pca
----------------------
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

df=pd.read_csv('/home/ise/1nt19is174/iris.csv')
df.head()

X=df.drop(['species'],axis=1)
x

Y=df['species']
Y

X_scaled = StandardScaler().fit_transform(X)
X_scaled[:5]

features = X_scaled.T
cov_matrix = np.cov(features)
cov_matrix[:5]
values, vectors = np.linalg.eig(cov_matrix)
values[:5]
vectors[:5]

explained_variances = []
for i in range(len(values)):
    explained_variances.append((values[i] / np.sum(values))*100)
print("variances of each feature",explained_variances)
plt.figure(figsize=(8,4))
plt.bar(range(4),explained_variances, alpha=0.6)
plt.ylabel('Percentage of explained variance')
plt.xlabel('Dimensions')
projected_1 = X_scaled.dot(vectors.T[0])
projected_2 = X_scaled.dot(vectors.T[1])
res = pd.DataFrame(projected_1, columns=['PC1'])
res['PC2'] = projected_2
res['Y'] = Y
res.head()
sns.FacetGrid(res, hue="Y", height=6).map(plt.scatter, 'PC1', 'PC2').add_legend()
plt.show()

--------------
6.logistic
-----------
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold

col_names=['x1','x2','result']
ds=pd.read_csv('/home/ise/1nt19is174/logistic.csv',header=None,names=col_names)

print(ds)
# input
x = ds.iloc[:, [0, 1]].values
  
# output
y = ds.iloc[:, 2].values

xp=preprocessing.scale(x)

kf=KFold(n_splits=5)
for train_index, test_index in kf.split(xp):
    xtrain, xtest, ytrain, ytest = train_test_split(xp, y, test_size = 0.20, random_state = 0)
    x1=xtrain[:,0]
    x2=xtrain[:,1]
    b0=0.0
    b1=0.0
    b2=0.0
    epoch=10000
    alpha=0.001
    while(epoch>0):
        for i in range(len(xtrain)):
            prediction=1/(1+np.exp(-(b0+b1*x1[i]+b2*x2[i])))
            b0=b0+alpha*(ytrain[i]-prediction)*prediction*(1-prediction)*1.0
            b1=b1+alpha*(ytrain[i]-prediction)*prediction*(1-prediction)*x1[i]
            b2=b2+alpha*(ytrain[i]-prediction)*prediction*(1-prediction)*x2[i]
        epoch=epoch-1
print(b0)
print(b1)
print(b2)
final_prediction=[]
x3=xtest[:,0]
x4=xtest[:,1]
print(ytest)
y_pred=[0]*len(xtest)
for i in range(len(xtest)):
    y_pred[i]=np.round(1/(1+np.exp(-(b0+b1*x3[i]+b2*x4[i]))))
    final_prediction.append(np.ceil(y_pred[i]))
print(final_prediction)    

from sklearn.metrics import accuracy_score
print("Accuracy",accuracy_score(ytest,y_pred))

------------------
7.linear
----------------
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import statistics as st

df=pd.read_csv("Food-Truck.csv",names=['a','b'])
x=df['a']
y=df['b']
x_mean=st.mean(x);
y_mean=st.mean(y)
plt.scatter(x,y)


n=0
d=0
for i in range(len(x)):
 n+=(x[i]-x_mean)*(y[i]-y_mean)
 d+=(x[i]-x_mean)**2
m=n/d
c=y_mean-m*x_mean
line=[]
for i in x:
 line.append(m*i+c)
plt.scatter(x,y,label='points',color='r')
plt.plot(x,line,label='line')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()


nm=st.mean(line)
sse=0
ssr=0
sst=0
for i in range(len(y)):
 sse+=(y[i]-line[i])**2
for i in range(len(line)):
 ssr+=(line[i]-nm)**2
for i in range(len(y)):
 sst+=(y[i]-y_mean)**2
r_sq=1-(sse/sst)
print(sse,ssr,sst,r_sq)

--------------
8.random forest
--------------
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

data=pd.read_csv(r"C:\Users\amana\Downloads\pima.csv")
data.head()

X = data.drop("Outcome", axis=1)
y = data["Outcome"]

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.20)

classifier = RandomForestClassifier(n_estimators=100)

classifier.fit(X_train, y_train)

RandomForestClassifier()

y_pred = classifier.predict(X_test)
print(accuracy_score(y_test,y_pred))

feature_importances_df = pd.DataFrame({"feature": list(X.columns), "importance":classifier.feature_importances_})
feature_importances_df

x=data.drop(['Outcome','SkinThickness'],axis=1)
y=data['Outcome']

x_tr,x_te,y_tr,y_te=train_test_split(x,y)
classifier.fit(x_tr,y_tr)
y_pred=classifier.predict(x_te)

print("accuracy: ", accuracy_score(y_te,y_pred))

from sklearn.tree import DecisionTreeClassifier
dt=DecisionTreeClassifier()
dt.fit(x_tr,y_tr)
y_pr=dt.predict(x_te)
print("accuracy: ",accuracy_score(y_te,y_pr))

------------------
k means
------------------
